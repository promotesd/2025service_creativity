{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/glm4-9bchat/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading data: 100%|██████████| 113/113 [00:00<00:00, 14290.43files/s]\n",
      "Generating train split: 6315233 examples [05:09, 20427.28 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['url', 'text', 'date', 'metadata'],\n",
      "        num_rows: 6315233\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "##open-web-math___open-web-math数据集\n",
    "dataset = load_dataset(r\"/root/.cache/huggingface/datasets/open-web-math___open-web-math\")\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'https://telescoper.wordpress.com/2010/11/23/bayes-and-hi-theorem/', 'text': 'Bayes and his\\xa0Theorem\\n\\nMy earlier post on Bayesian probability seems to have generated quite a lot of readers, so this lunchtime I thought I’d add a little bit of background. The previous discussion started from the result\\n\\n$P(B|AC) = K^{-1}P(B|C)P(A|BC) = K^{-1} P(AB|C)$\\n\\nwhere\\n\\n$K=P(A|C).$\\n\\nAlthough this is called Bayes’ theorem, the general form of it as stated here was actually first written down, not by Bayes but by Laplace. What Bayes’ did was derive the special case of this formula for “inverting” the binomial distribution. This distribution gives the probability of x successes in n independent “trials” each having the same probability of success, p; each “trial” has only two possible outcomes (“success” or “failure”). Trials like this are usually called Bernoulli trials, after Daniel Bernoulli. If we ask the question “what is the probability of exactly x successes from the possible n?”, the answer is given by the binomial distribution:\\n\\n$P_n(x|n,p)= C(n,x) p^x (1-p)^{n-x}$\\n\\nwhere\\n\\n$C(n,x)= n!/x!(n-x)!$\\n\\nis the number of distinct combinations of x objects that can be drawn from a pool of n.\\n\\nYou can probably see immediately how this arises. The probability of x consecutive successes is p multiplied by itself x times, or px. The probability of (n-x) successive failures is similarly (1-p)n-x. The last two terms basically therefore tell us the probability that we have exactly x successes (since there must be n-x failures). The combinatorial factor in front takes account of the fact that the ordering of successes and failures doesn’t matter.\\n\\nThe binomial distribution applies, for example, to repeated tosses of a coin, in which case p is taken to be 0.5 for a fair coin. A biased coin might have a different value of p, but as long as the tosses are independent the formula still applies. The binomial distribution also applies to problems involving drawing balls from urns: it works exactly if the balls are replaced in the urn after each draw, but it also applies approximately without replacement, as long as the number of draws is much smaller than the number of balls in the urn. I leave it as an exercise to calculate the expectation value of the binomial distribution, but the result is not surprising: E(X)=np. If you toss a fair coin ten times the expectation value for the number of heads is 10 times 0.5, which is five. No surprise there. After another bit of maths, the variance of the distribution can also be found. It is np(1-p).\\n\\nSo this gives us the probability of x given a fixed value of p. Bayes was interested in the inverse of this result, the probability of p given x. In other words, Bayes was interested in the answer to the question “If I perform n independent trials and get x successes, what is the probability distribution of p?”. This is a classic example of inverse reasoning. He got the correct answer, eventually, but by very convoluted reasoning. In my opinion it is quite difficult to justify the name Bayes’ theorem based on what he actually did, although Laplace did specifically acknowledge this contribution when he derived the general result later, which is no doubt why the theorem is always named in Bayes’ honour.\\n\\nThis is not the only example in science where the wrong person’s name is attached to a result or discovery. In fact, it is almost a law of Nature that any theorem that has a name has the wrong name. I propose that this observation should henceforth be known as Coles’ Law.\\n\\nSo who was the mysterious mathematician behind this result? Thomas Bayes was born in 1702, son of Joshua Bayes, who was a Fellow of the Royal Society (FRS) and one of the very first nonconformist ministers to be ordained in England. Thomas was himself ordained and for a while worked with his father in the Presbyterian Meeting House in Leather Lane, near Holborn in London. In 1720 he was a minister in Tunbridge Wells, in Kent. He retired from the church in 1752 and died in 1761. Thomas Bayes didn’t publish a single paper on mathematics in his own name during his lifetime but despite this was elected a Fellow of the Royal Society (FRS) in 1742. Presumably he had Friends of the Right Sort. He did however write a paper on fluxions in 1736, which was published anonymously. This was probably the grounds on which he was elected an FRS.\\n\\nThe paper containing the theorem that now bears his name was published posthumously in the Philosophical Transactions of the Royal Society of London in 1764.\\n\\nP.S. I understand that the authenticity of the picture is open to question. Whoever it actually is, he looks\\xa0 to me a bit like Laurence Olivier…\\n\\n11 Responses to “Bayes and his\\xa0Theorem”\\n\\n1. Bryn Jones Says:\\n\\nThe Royal Society is providing free access to electronic versions of its journals until the end of this month. Readers of this blog might like to look at Thomas Bayes’s two posthumous publications in the Philosophical Transactions.\\n\\nThe first is a short paper about series. The other is the paper about statistics communicated by Richard Price. (The statistics paper may be accessible on a long-term basis because it is one of the Royal Society’s Trailblazing papers the society provides access to as part of its 350th anniversary celebrations.)\\n\\nIncidentally, both Thomas Bayes and Richard Price were buried in the Bunhill Fields Cemetery in London and their tombs can be seen there today.\\n\\n2. Steve Warren Says:\\n\\nYou may be remembered in history as the discoverer of coleslaw, but you weren’t the first.\\n\\n• Anton Garrett Says:\\n\\nFor years I thought it was “cold slaw” because it was served cold. A good job I never asked for warm slaw.\\n\\n3. telescoper Says:\\n\\nMy surname, in Spanish, means “Cabbages”. So it was probably one of my ancestors who invented the chopped variety.\\n\\n4. Anton Garrett Says:\\n\\nThomas Bayes is now known to have gone to Edinburgh University, where his name appears in the records. He was barred from English universities because his nonconformist family did not have him baptised in the Church of England. (Charles Darwin’s nonconformist family covered their bets by having baby Charles baptised in the CoE, although perhaps they believed it didn’t count as a baptism since Charles had no say in it. Tist is why he was able to go to Christ’s College, Cambridge.)\\n\\n5. “Cole” is an old English word for cabbage, which survives in “cole slaw”. The German word is “Kohl”. (Somehow, I don’t see PM or President Cabbage being a realistic possibility. 🙂 )\\n\\nNote that Old King Cole is unrelated (etymologically). Of course, this discussion could cause Peter to post a clip of\\n\\nNat “King” Cole\\n(guess what his real surname is).\\n\\nTo remind people to pay attention to spelling when they hear words, we’ll close with the Quote of the Day:\\n\\nIt’s important to pay close attention in school. For years I thought that\\nbears masturbated all winter.\\n\\n—Damon R. Milhem\\n\\n6. Of course, this discussion could cause Peter to post a clip of\\nNat King Cole\\n(giess what his real surname is).\\n\\n7. Of course, this discussion could cause Peter to post a clip of\\nNat King Cole\\n(giess what his real surname is).\\n\\nThe first typo was my fault; the extra linebreaks in the second attempt\\n(tested again here) appear to be a new “feature”.\\n\\n8. telescoper Says:\\n\\nThe noun “cole” can be found in English dictionaries as a generic name for plants of the cabbage family. It is related to the German kohl and scottish kail or kale. These are all derived from the latin word colis (or caulis) meaning a stem, which is also the root of the word cauliflower.\\n\\nThe surname “Cole” and the variant “Coles” are fairly common in England and Wales, but are not related to the latin word for cabbage. Both are diminutives of the name “Nicholas”.\\n\\n9. […] I posted a little piece about Bayesian probability. That one and the others that followed it (here and here) proved to be surprisingly popular so I’ve been planning to add a few more posts […]\\n\\n10. It already has a popular name: Stigler’s law of eponymy.', 'date': '2016-09-28 22:12:05', 'metadata': '{\"extraction_info\": {\"found_math\": true, \"script_math_tex\": 0, \"script_math_asciimath\": 0, \"math_annotations\": 0, \"math_alttext\": 0, \"mathml\": 0, \"mathjax_tag\": 0, \"mathjax_inline_tex\": 0, \"mathjax_display_tex\": 0, \"mathjax_asciimath\": 0, \"img_math\": 4, \"codecogs_latex\": 0, \"wp_latex\": 0, \"mimetex.cgi\": 0, \"/images/math/codecogs\": 0, \"mathtex.cgi\": 0, \"katex\": 0, \"math-container\": 0, \"wp-katex-eq\": 0, \"align\": 0, \"equation\": 0, \"x-ck12\": 0, \"texerror\": 0, \"math_score\": 0.6193313002586365, \"perplexity\": 1089.0032368849284}, \"config\": {\"markdown_headings\": false, \"markdown_code\": true, \"boilerplate_config\": {\"ratio_threshold\": 0.18, \"absolute_threshold\": 10, \"end_threshold\": 15, \"enable\": true}, \"remove_buttons\": true, \"remove_image_figures\": true, \"remove_link_clusters\": true, \"table_config\": {\"min_rows\": 2, \"min_cols\": 3, \"format\": \"plain\"}, \"remove_chinese\": true, \"remove_edit_buttons\": true, \"extract_latex\": true}, \"warc_path\": \"s3://commoncrawl/crawl-data/CC-MAIN-2016-40/segments/1474738661768.10/warc/CC-MAIN-20160924173741-00020-ip-10-143-35-109.ec2.internal.warc.gz\"}'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glm4-9bchat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
